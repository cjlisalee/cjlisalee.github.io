
<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Chi-Jung Lee</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
	</head>
    <body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="../index.html">Home</a></li>
					<li><a href="mailto:cjlisalee@cmlab.csie.ntu.edu.tw">Contact</a></li>
				</ul>
			</nav>     
            
        <!-- Project -->
            <article id="project" class="wrapper style3">
				<div class="container">
                    <article class="box style3">
                        <div class="col-12">
                            <div class="col-8 col-7-large col-12-medium">
                                <h3><a href="#">Dual-MR: Interaction with Mixed Reality Using Smartphones</a></h3> 
                                <span><h5>VRST '18 (Best Poster Award)</h5></span>
                                    <p><strong>Chi-Jung Lee</strong>, Hung-Kuo Chu</p>
                                    <p>Mixed reality (MR) has changed the perspective we see and interact with our world. While the current-generation of MR head-mounted devices (HMDs) are capable of generating high quality visual contents, interation in most MR applications typically relies on in-air hand gestures, gaze, or voice. These interfaces although are intuitive to learn, may easily lead to inaccurate operations due to fatigue or constrained by the environment. In this work, we present Dual-MR, a novel MR interation system that i) synchronizes the MR viewpoints of HMD and handheld smartphone, and ii) enables precise, tactile, immersive and user-friendly object-level manipulations throught the multi-touch input of smartphone. In addition, Dual-MR allows multiple users to join the same MR coordinate system to facilite the collaborate in the same physical space, which further broadens its usability. A preliminary user study shows that our system easily overwhelms the conventional interface, which combines in-air hand gesture and gaze, in the completion time for a series of 3D object manipulation tasks in MR.</p>
                                    <p>[<a href="../documents/Dual-MR.pdf">Paper</a>] [<a href="https://youtu.be/Pk59MvPkdu4">Video</a>] [<a href="https://dl.acm.org/doi/10.1145/3281505.3281618">Digital Library</a>]</p>

                                    <iframe width="1120" height="630" src="https://www.youtube.com/embed/Pk59MvPkdu4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    </article>
                </div>
                        
            </article>
    </body>
</html>
